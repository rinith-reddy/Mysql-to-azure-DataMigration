{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46c5d2e4-a2b9-4040-a935-d91dc417611e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12789123-e6c0-4970-9058-2e34b3d96b86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### reading the files from bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d315541-573c-400c-ac35-03cdd356fb19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_path = \"abfss://bronze@migrationstoragerinith.dfs.core.windows.net/second_method/\"\n",
    "silver_path = \"abfss://silver@migrationstoragerinith.dfs.core.windows.net/second_method/\"\n",
    "\n",
    "paths = {\n",
    "    \"products\": bronze_path + \"products\",\n",
    "    \"customers\": bronze_path + \"customers\",\n",
    "    \"dates\": bronze_path + \"dates\",\n",
    "    \"orders\": bronze_path + \"orders\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea667eb5-2d8f-4179-8c86-419d9f9fe9b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean_colnames(df):\n",
    "    rename_dict = {col: re.sub(r\"[^\\w]\", \"_\", col.strip().lower()) for col in df.columns}\n",
    "    for old_col, new_col in rename_dict.items():\n",
    "        df = df.withColumnRenamed(old_col, new_col)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Clean and standardize string columns\n",
    "def clean_string_columns(df):\n",
    "    for col_name, dtype in df.dtypes:\n",
    "        if dtype == 'string':\n",
    "            df = df.withColumn(col_name, regexp_replace(trim(lower(col(col_name))), r'[^a-zA-Z0-9\\s]', ''))\n",
    "    return df\n",
    "\n",
    "# Replace nulls in selected columns\n",
    "def fill_missing(df, fill_dict):\n",
    "    return df.fillna(fill_dict)\n",
    "\n",
    "# Remove duplicates based on key columns\n",
    "def remove_duplicates(df, subset_cols):\n",
    "    return df.dropDuplicates(subset=subset_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "441bf92e-ed1b-434a-ac4b-464fbf147530",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### products table cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0152c1de-2404-4767-8585-79abff43c6d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_products = spark.read.format(\"csv\").option(\"inferschema\",True).option(\"header\", True).load(paths[\"products\"])\n",
    "\n",
    "# Standardize column names\n",
    "df_products = clean_colnames(df_products)\n",
    "\n",
    "# Drop rows with null primary key\n",
    "df_products = df_products.dropna(subset=[\"product_id\"])\n",
    "\n",
    "# Cast to proper types\n",
    "df_products = df_products \\\n",
    "    .withColumn(\"product_id\", col(\"product_id\").cast(\"int\")) \\\n",
    "    .withColumn(\"price\", col(\"price\").cast(\"float\")) \\\n",
    "    .withColumn(\"weight_lbs\", col(\"weight_lbs\").cast(\"float\")) \\\n",
    "    .withColumn(\"in_stock\", col(\"in_stock\").cast(\"boolean\"))\n",
    "\n",
    "# Fill missing brand with default\n",
    "df_products = fill_missing(df_products, {\"brand\": \"Unknown\"})\n",
    "\n",
    "# Clean all string columns\n",
    "df_products = clean_string_columns(df_products)\n",
    "\n",
    "# Cap unreasonable price outliers\n",
    "df_products = df_products.withColumn(\"price\", when(col(\"price\") > 5000.0, 5000.0).otherwise(col(\"price\")))\n",
    "\n",
    "# ------------------------\n",
    "# Final Select and Save\n",
    "# ------------------------\n",
    "\n",
    "df_products = df_products.select(\n",
    "    \"product_id\",\n",
    "    \"product_name\",\n",
    "    \"category\",\n",
    "    \"brand\",\n",
    "    \"price\",\n",
    "    \"sku\",\n",
    "    \"weight_lbs\",\n",
    "    \"dimensions\",\n",
    "    \"in_stock\"\n",
    ").dropDuplicates([\"product_id\"])\n",
    "\n",
    "df_products.write.format(\"delta\").mode(\"append\").save(silver_path + \"dim_products\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8176e7e9-cb4b-4f10-b8bc-b1cfcd321202",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_products.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1d10fc0-61fc-4938-a689-18cf7f1c1056",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### After tranfromation move the bronze file to archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51c60d19-cdc3-488d-988e-1a5fbf449918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "today = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "base_path = \"abfss://bronze@migrationstoragerinith.dfs.core.windows.net/second_method/products\"\n",
    "archive_path = f\"{base_path}/archive/{today}\"\n",
    "\n",
    "files = [f.path for f in dbutils.fs.ls(base_path) if not f.isDir() and \"archive/\" not in f.path]\n",
    "\n",
    "# Move each file to archive\n",
    "for file in files:\n",
    "    filename = file.split(\"/\")[-1]\n",
    "    destination = f\"{archive_path}/{filename}\"\n",
    "    dbutils.fs.mv(file, destination)\n",
    "    print(f\"Moved: {filename} → {destination}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9540317-56e5-42f7-86fb-e1f7d09b8820",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ed6e1d6-823a-47ed-943e-e36c212aebbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### customers table cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dc0e9eb-baab-4488-8ab7-14878994ecac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_customers = spark.read.option(\"header\", True).csv(paths[\"customers\"])\n",
    "df_customers.printSchema()\n",
    "df_customers.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6479766-f97f-4cfa-94ca-625cc1ba15b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61b509db-2805-481a-83b1-58f21c79c6d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_customers = spark.read.option(\"header\", True).csv(paths[\"customers\"])\n",
    "\n",
    "df_customers = clean_colnames(df_customers)\n",
    "\n",
    "# Drop records missing customer_id\n",
    "df_customers = df_customers.filter(col(\"customer_id\").rlike(\"^[0-9]+$\"))\n",
    "\n",
    "df_customers = df_customers.dropna(subset=[\"customer_id\"])\n",
    "\n",
    "# Convert types\n",
    "df_customers = df_customers \\\n",
    "    .withColumn(\"customer_id\", col(\"customer_id\").cast(\"int\")) \\\n",
    "    .withColumn(\"zip_code\", col(\"zip_code\").cast(\"int\")) \\\n",
    "    .withColumn(\"age\", col(\"age\").cast(\"int\")) \\\n",
    "    .withColumn(\"annual_income\", col(\"annual_income\").cast(\"float\")) \\\n",
    "    .withColumn(\"customer_since\", to_date(col(\"customer_since\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "gender_map = {'m': 'Male', 'male': 'Male', 'f': 'Female', 'female': 'Female'}\n",
    "df_customers = df_customers.withColumn(\"gender\", lower(col(\"gender\")))\n",
    "df_customers = df_customers.replace(gender_map, subset=[\"gender\"])\n",
    "\n",
    "df_customers = df_customers.withColumn(\"age\", when((col(\"age\") < 18) | (col(\"age\") > 90), None).otherwise(col(\"age\")))\n",
    "\n",
    "\n",
    "\n",
    "df_customers=df_customers.withColumn(\"annual_income\", col(\"annual_income\").cast(\"float\"))\n",
    "\n",
    "df_customers = df_customers.withColumn(\"annual_income\", when(col(\"annual_income\") < 10000.0, 10000)\n",
    "                                                       .when(col(\"annual_income\") > 50000.0, 500000)\n",
    "                                                       .otherwise(col(\"annual_income\")))\n",
    "\n",
    "df_customers = remove_duplicates(df_customers, [\"first_name\", \"last_name\", \"email\"])\n",
    "\n",
    "df_customers = fill_missing(df_customers, {\"phone\": \"Not Provided\"})\n",
    "\n",
    "df_customers = clean_string_columns(df_customers)\n",
    "\n",
    "df_customers.write.format(\"delta\").mode(\"append\").save(silver_path + \"dim_customers\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd9b8e85-e239-429f-aee7-3795aa4bcb9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### after the tranformation we wrtiting the bronze file to archive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfe45e03-7f56-4179-8606-dbb765d42278",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "today = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "base_path = \"abfss://bronze@migrationstoragerinith.dfs.core.windows.net/second_method/customers\"\n",
    "archive_path = f\"{base_path}/archive/{today}\"\n",
    "\n",
    "files = [f.path for f in dbutils.fs.ls(base_path) if not f.isDir() and \"archive/\" not in f.path]\n",
    "\n",
    "# Move each file to archive\n",
    "for file in files:\n",
    "    filename = file.split(\"/\")[-1]\n",
    "    destination = f\"{archive_path}/{filename}\"\n",
    "    dbutils.fs.mv(file, destination)\n",
    "    print(f\"Moved: {filename} → {destination}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6cc40b0-aabd-4eda-8df3-0a3e3d6ef4a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### tables table cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf0c674c-693d-4e71-91c2-5f39f066f959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_dates = spark.read.option(\"header\", True).csv(paths[\"dates\"])\n",
    "df_dates = clean_colnames(df_dates)\n",
    "\n",
    "df_dates = df_dates.withColumn(\"date_id\", col(\"date_id\").cast(\"int\")) \\\n",
    "                   .withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\")) \\\n",
    "                   .withColumn(\"year\", col(\"year\").cast(\"int\")) \\\n",
    "                   .withColumn(\"month\", col(\"month\").cast(\"int\")) \\\n",
    "                   .withColumn(\"week\", col(\"week\").cast(\"int\")) \\\n",
    "                   .withColumn(\"day\", col(\"day\").cast(\"int\")) \\\n",
    "                   .withColumn(\"day_of_week\", col(\"day_of_week\").cast(\"int\")) \\\n",
    "                   .withColumn(\"is_weekend\", col(\"is_weekend\").cast(\"boolean\")) \\\n",
    "                   .withColumn(\"is_holiday\", col(\"is_holiday\").cast(\"boolean\"))\n",
    "\n",
    "# Clean strings\n",
    "df_dates = clean_string_columns(df_dates)\n",
    "\n",
    "# Save cleaned data\n",
    "df_dates.write.format(\"delta\").mode(\"append\").save(silver_path + \"dim_dates\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebadcebd-843e-4ff2-8153-88bb7a92c1c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### after the tranformation we wrtiting the bronze file to archive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0359d65c-744b-45cd-bc4e-3ade4b94cf09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "today = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "base_path = \"abfss://bronze@migrationstoragerinith.dfs.core.windows.net/second_method/dates\"\n",
    "archive_path = f\"{base_path}/archive/{today}\"\n",
    "\n",
    "files = [f.path for f in dbutils.fs.ls(base_path) if not f.isDir() and \"archive/\" not in f.path]\n",
    "\n",
    "for file in files:\n",
    "    filename = file.split(\"/\")[-1]\n",
    "    destination = f\"{archive_path}/{filename}\"\n",
    "    dbutils.fs.mv(file, destination)\n",
    "    print(f\"Moved: {filename} → {destination}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed8cb619-b9c0-47cb-8c1d-29e8cd3c601a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### orders table cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0b847c1-1ab5-4882-8357-19c689dce0cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_orders = spark.read.option(\"header\", True).csv(paths[\"orders\"])\n",
    "df_orders = clean_colnames(df_orders)\n",
    "\n",
    "df_orders = df_orders.dropna(subset=[\"order_id\", \"customer_id\", \"product_id\"])\n",
    "\n",
    "conversion_types = {\n",
    "    \"order_id\": \"int\", \"customer_id\": \"int\", \"product_id\": \"int\", \"date_id\": \"int\", \"store_id\": \"int\",\n",
    "    \"quantity\": \"int\", \"unit_price\": \"float\", \"line_total\": \"float\", \"order_subtotal\": \"float\",\n",
    "    \"discount_percent\": \"float\", \"discount_amount\": \"float\", \"tax_rate\": \"float\", \"tax_amount\": \"float\",\n",
    "    \"shipping_cost\": \"float\", \"total_amount\": \"float\"\n",
    "}\n",
    "for col_name, dtype in conversion_types.items():\n",
    "    df_orders = df_orders.withColumn(col_name, col(col_name).cast(dtype))\n",
    "\n",
    "df_orders = df_orders.withColumn(\"payment_method\", lower(trim(col(\"payment_method\")))) \\\n",
    "                     .withColumn(\"order_status\", lower(trim(col(\"order_status\"))))\n",
    "\n",
    "payment_map = {\n",
    "    \"credit card\": \"Credit Card\", \"cash\": \"Cash\", \"paypal\": \"PayPal\", \"gift card\": \"Gift Card\", \"debit card\": \"Debit Card\", \"debit\": \"Debit\"\n",
    "}\n",
    "status_map = {\n",
    "    \"completed\": \"Completed\", \"shipped\": \"Shipped\", \"cancelled\": \"Cancelled\", \"returned\": \"Returned\", \"processing\": \"Processing\"\n",
    "}\n",
    "df_orders = df_orders.replace(payment_map, subset=[\"payment_method\"])\n",
    "df_orders = df_orders.replace(status_map, subset=[\"order_status\"])\n",
    "\n",
    "df_orders = df_orders.withColumn(\"quantity\", when((col(\"quantity\") <= 0) | (col(\"quantity\") > 100), None).otherwise(col(\"quantity\")))\n",
    "\n",
    "df_orders = clean_string_columns(df_orders)\n",
    "\n",
    "df_orders.write.format(\"delta\").mode(\"append\").save(silver_path + \"fact_orders\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fcfee98-5572-46be-a146-79d05897d8fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "today = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "base_path = \"abfss://bronze@migrationstoragerinith.dfs.core.windows.net/second_method/orders\"\n",
    "archive_path = f\"{base_path}/archive/{today}\"\n",
    "\n",
    "files = [f.path for f in dbutils.fs.ls(base_path) if not f.isDir() and \"archive/\" not in f.path]\n",
    "\n",
    "# Move each file to archive\n",
    "for file in files:\n",
    "    filename = file.split(\"/\")[-1]\n",
    "    destination = f\"{archive_path}/{filename}\"\n",
    "    dbutils.fs.mv(file, destination)\n",
    "    print(f\"Moved: {filename} → {destination}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sliver_secondmethod",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
